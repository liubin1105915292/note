下载地址:
http://www.apache.org/dist/hadoop/core/
解压
修改配置文件 /etc/hadoop
1.hadoop-env.sh文件
export JAVA_HOME=${JAVA_HOME}  修改为java路径
2.core-site.xml 文件
<configuration>
  <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop001:9000/</value>
 </property>
 <property>
         <name>hadoop.tmp.dir</name>
          <value>/home/hadoop/hadoop2.6/hadoop-2.6.5/data</value>
   </property>
</configuration>

3.hdfs-site.xml文件

<configuration>
 <property>
         <name>dfs.name.dir</name>
          <value>/usr/local/data/namenode</value>
   </property>
 <property>
         <name>dfs.data.dir</name>
          <value>/usr/local/data/datanode</value>
   </property>
 <property>
         <name>hadoop.tmp.dir</name>
          <value>/home/hadoop/hadoop2.6/hadoop-2.6.5/data</value>
   </property>
 <property>
	<name>dfs.replication</name>   备份数量
	<value>1</value>
</property>
<configuration>

4.修改名称
mv mapred-site.xml.template mapred-site.xml
vi mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

5.配置 vi yarn-site.xml
<configuration>

<!-- Site specific YARN configuration properties -->
<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop001</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>

如果多台节点还要配置slaves
6.配置环境变量
export JAVA_HOME=/home/hadoop/java/java8/jdk1.8.0_171
export HADOOP_HOME=/home/hadoop/hadoop2.6/hadoop-2.6.5
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

7.在bin目录下格式化
hadoop namenode -format
会格式化namenode

8.启动hadoop
在sbin目录下
start_dfs.sh  启动hdfs
atart_yarn.sh 启动yarn

jps查看所有进程

9.访问
http://hadoop001:50070/
http://hadoop001:8088/cluster

10:上传东西到hdfs
hadoop fs -put 资源 hdfs://hadoop001:9000/                             存放路径自己设置
hadoop fs -get  hdfs://hadoop001:9000/资源   下载资源到本地
hadoop fs mkdir hdfs://hadoop001:9000/wordcount[可缩写/wordcount]  创建目录

11.运行mapreduce    到share/hadoop/mapreduce目录下
hadoop jar hadoop-mapreduce-examples-2.6.5.jar  pi 5 5   使用示例计算pi,后面值越大精度越大
hadoop jar hadoop-mapreduce-examples-2.6.5.jar  wordcount /wordcount/input /wordcount/output  计算wordcount,在input目录下存在test.txt

11.ssh免密码登陆设置
命令:ssh-keygen -t rsa   产生秘钥使用rsa加密算法  然后回车
(/home/liubin/.ssh/id_rsa)默认地址  .ssh是隐藏文件  使用-a显示
将.pub公钥放入另一台服务器  使用远程拷贝命令 scp id_rsa.pub 主机名:/目录
将公钥加入授权列表 authorized_keys  没有自己创建 文件权限必须是600
将公钥追加到authorized_keys文件:cat ../id_rsa.pub >> ./authorized_keys

二、如果还是不行
 vi /etc/ssh/sshd_config 
 RSAAuthentication yes # 启用 RSA 认证
 PubkeyAuthentication yes # 启用公钥私钥配对认证方式
 AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径
service sshd restart 

启动hdfs无密码只需要将自己的公钥追加的授权列表

